{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicación del código**\n",
    "\n",
    "El código de Python que se muestra a continuación extrae información de juegos de mesa del sitio web \"https://www.ludonauta.es\" a través de Selenium y BeautifulSoup. \n",
    "\n",
    "Utiliza listas de páginas y funciones para obtener detalles sobre cada juego, incluyendo nombre, fecha de publicación, autores, categorías, mecánicas, puntuación, precios y disponibilidad en tiendas. \n",
    "\n",
    "Finalmente, se crean dos archivos JSON para guardar la información de juegos y precios, y se incorporan en dos DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Desafíos y resolución**\n",
    "\n",
    "Se presentaron desafíos durante el web scraping al intentar extraer la información, se abordaron mediante soluciones específicas que detallo a continuación:\n",
    "-\tEn el HTML muchas de las etiquetas estaban nombradas igual, Se sacaron las etiquetas anteriores o posteriores que las contenían, y luego utilizar los métodos FIND : NEXT y PREVIOUS SIBLING para encontrar los elementos buscados.\n",
    "\n",
    "-\tAl extraer los datos se hizo limpieza, eliminando tabulaciones o mediante expresiones regulares para encontrar patrones.\n",
    "\n",
    "-\tCuando se extrajeron los precios, también se extraían los nombres de las tiendas y la disponibilidad de los juegos relacionados que aparecían abajo, por eso se mapearon las tiendas con el tamaño de los precios. \n",
    "\n",
    "-\tSe guardaron los datos de los precios en listas  y no en diccionarios, para extraer todos los precios de venta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LISTADO DE LAS PÁGINAS \n",
    "\n",
    "A continuación, se crea un listado completo de las páginas concatenando el sitio base con números del 1 al 379 (número total de páginas que hay en la web en el momento de extracción)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_ludonauta= \"https://www.ludonauta.es\"\n",
    "\n",
    "listado_paginas= []\n",
    "for numero_pagina in range(1,380): \n",
    "    todas_paginas= f\"{url_ludonauta}/juegos-mesas/listar/page:{numero_pagina}\"\n",
    "    numero_pagina += 1\n",
    "    listado_paginas.append(todas_paginas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACCIÓN DEL LINK DE LOS JUEGOS DE CADA PAGINA\n",
    "\n",
    "La función extraer_href_pagina recibe un objeto BeautifulSoup (soup) que representa la página web de cada juego. Busca todos los enlaces de juegos con la clase \"product-name\", construye las URL completas al agregarles el prefijo url_ludonauta, y devuelve la lista de URL de cada juego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_href_pagina(soup):\n",
    "    \"\"\"Crea una lista de los enlaces_juegos de cada página\"\"\"\n",
    "    juegos_href = soup.find_all(\"a\", class_=\"product-name\")\n",
    "\n",
    "    enlaces_juegos = []\n",
    "\n",
    "    for juego in juegos_href:\n",
    "        enlace = juego.get('href')\n",
    "        enlace_entero= url_ludonauta + enlace\n",
    "\n",
    "        enlaces_juegos.append(enlace_entero)\n",
    "    \n",
    "    return enlaces_juegos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACCIÓN DE LA INFORMACIÓN DE CADA JUEGO \n",
    "\n",
    "\n",
    "La función extraer_juego(soup), recibe un objeto BeautifulSoup que representa la web de Ludonauta. Luego, utiliza métodos de búsqueda para extraer información específica sobre el juego. La información se organiza en un diccionario llamado json_juego, que luego se devuelve como resultado de la función. Si algún dato no está disponible, se asigna el valor np.nan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_juego(soup):\n",
    "    \"\"\"Extrae de los juegos: Nombre, Fecha publicación, Autores, Categorías, \n",
    "    Mecánicas, Puntuación, Jugadores, J.Mínimos y J.Máximos (si no los hay, son Jugadores), \n",
    "    Duración, D.Mínima y D.Máxima (si no la hay, es Duración), Edad, Complejidad y Dep idioma,\n",
    "    y lo guarda en json_juego\"\"\"\n",
    "\n",
    "    try:\n",
    "        nombre_juego= soup.find(\"h2\").text\n",
    "    except:\n",
    "        nombre_juego= np.nan\n",
    "    try:\n",
    "        puntuacion= soup.find(\"span\", class_=\"text-navy\").text\n",
    "    except:\n",
    "        puntuacion= np.nan\n",
    "    try:\n",
    "        contenedor_fecha = soup.find('dt', string='Fecha pub.')\n",
    "        etiquetas_dentro_fecha = contenedor_fecha.find_next_sibling('dd').find_all('p')\n",
    "        for etiqueta in etiquetas_dentro_fecha:\n",
    "            fecha_publicacion= etiqueta.text\n",
    "    except:\n",
    "        fecha_publicacion= np.nan\n",
    "    try:\n",
    "        contenedor_autores = soup.find('dt', string='Autores')\n",
    "        etiquetas_dentro_autores = contenedor_autores.find_next_sibling('dd').find_all('p')\n",
    "        for etiqueta in etiquetas_dentro_autores:\n",
    "            autores_sin_limpiar= etiqueta.text.split(\"|\")\n",
    "            autores = [autor.strip() for autor in autores_sin_limpiar]\n",
    "    except:\n",
    "        autores= np.nan\n",
    "    try:\n",
    "        contenedor_categorias = soup.find('dt', string='Categorías')\n",
    "        etiquetas_dentro_categorias = contenedor_categorias.find_next_sibling('dd').find_all('p')\n",
    "        for etiqueta in etiquetas_dentro_categorias:\n",
    "            categorias_sin_limpiar= etiqueta.text.split(\"|\")\n",
    "            categorias = [categoria.strip() for categoria in categorias_sin_limpiar]\n",
    "    except:\n",
    "        categorias= np.nan\n",
    "    try:\n",
    "        contenedor_mecanicas = soup.find('dt', string='Mecánicas')\n",
    "        etiquetas_dentro_mecanicas = contenedor_mecanicas.find_next_sibling('dd').find_all('p')\n",
    "        for etiqueta in etiquetas_dentro_mecanicas:\n",
    "            mecanicas_sin_limpiar= etiqueta.text.split(\"|\")\n",
    "            mecanicas = [mecanica.strip() for mecanica in mecanicas_sin_limpiar]\n",
    "    except:\n",
    "        mecanicas= np.nan\n",
    "    try:\n",
    "        contenedor_complejidad = soup.find('dt', string='Complejidad')\n",
    "        etiquetas_dentro_complejidad = contenedor_complejidad.find_next_sibling('dd').find_all('span', class_='label label-navy')\n",
    "        # COMPLEJIDAD (1: muy baja, 2: baja, 3: media, 4: alta, 5: muy alta)\n",
    "        complejidad= len(etiquetas_dentro_complejidad)\n",
    "    except:\n",
    "        complejidad= np.nan\n",
    "    try:\n",
    "        contenedor_idioma = soup.find('dt', string='Dep. idioma')\n",
    "        etiquetas_dentro_idioma = contenedor_idioma.find_next_sibling('dd').find_all('span', class_='label label-navy')\n",
    "        # DEP IDIOMA (1: muy baja, 2: baja, 3: media, 4: alta, 5: muy alta)\n",
    "        dep_idioma= len(etiquetas_dentro_idioma)\n",
    "    except:\n",
    "        dep_idioma= np.nan\n",
    "    try:\n",
    "        contenedor_jugadores = soup.find('small', string='jugadores')\n",
    "        etiquetas_dentro_jugadores = contenedor_jugadores.find_previous_sibling('div').text\n",
    "        jugadores_lista = [x.replace('\\t', '').strip() for x in etiquetas_dentro_jugadores.split(\"\\n\") if x.strip() != \"\"]\n",
    "        jugadores= str(jugadores_lista[0])\n",
    "    except:\n",
    "        jugadores= np.nan\n",
    "    try:\n",
    "        jugadores_minimos = [elemento.split('-')[0] for elemento in jugadores_lista]\n",
    "        jugadores_minimos= (int(jugadores_minimos[0]))\n",
    "    except:\n",
    "        jugadores_minimos= np.nan\n",
    "    try:\n",
    "        jugadores_maximos = jugadores_lista[0].split('-')[1] if '-' in jugadores_lista[0] else jugadores_minimos\n",
    "        jugadores_maximos = int(jugadores_maximos)\n",
    "    except:\n",
    "        jugadores_maximos= np.nan\n",
    "    try:\n",
    "        contenedor_duracion = soup.find('small', string='minutos')\n",
    "        etiquetas_dentro_duracion = contenedor_duracion.find_previous_sibling('div').text\n",
    "        juego = [x.replace('\\t', '').strip() for x in etiquetas_dentro_duracion.split(\"\\n\") if x.strip() != \"\"]\n",
    "        duracion_juego= str(juego[0])\n",
    "        if duracion_juego == \"-\":\n",
    "            duracion_juego= np.nan\n",
    "    except:\n",
    "        duracion_juego= np.nan\n",
    "    try:\n",
    "        duracion_minima = [elemento.split('-')[0] for elemento in juego]\n",
    "        duracion_minima= int(duracion_minima[0])\n",
    "    except:\n",
    "        duracion_minima= np.nan\n",
    "    try:\n",
    "        duracion_maxima = juego[0].split('-')[1] if '-' in juego[0] else duracion_minima\n",
    "        duracion_maxima = int(duracion_maxima)\n",
    "    except:\n",
    "        duracion_maxima= np.nan\n",
    "    try:\n",
    "        contenedor_edad = soup.find('small', string='años')\n",
    "        etiquetas_dentro_edad = contenedor_edad.find_previous_sibling('div').text\n",
    "        edad = [x.replace('\\t', '').strip().replace(\"+\", \"\") for x in etiquetas_dentro_edad.split(\"\\n\") if x.strip() != \"\"]\n",
    "        edad= int(edad[0])\n",
    "    except:\n",
    "        edad= np.nan\n",
    "    try:\n",
    "        ediciones_juegos = soup.find(\"div\", class_= \"product-type small\").text\n",
    "        limpieza_elementos_juego = [x.replace('\\t', '').strip() for x in ediciones_juegos.split(\"\\n\") if x.strip() != \"\"]\n",
    "        str_ediciones_juegos= str(limpieza_elementos_juego[0])\n",
    "    except:\n",
    "        edicion_juegos= np.nan\n",
    "        juegos_relacionados = np.nan\n",
    "    try:\n",
    "        edicion_juegos= str_ediciones_juegos.split(\" «\")[0]\n",
    "    except:\n",
    "        edicion_juegos= np.nan\n",
    "    try:        \n",
    "        juegos_relacionados= re.findall(r\"«(.*?)»\", str_ediciones_juegos)\n",
    "        # no entraba en except porque cuando no había nada era una lista vacía\n",
    "        if not juegos_relacionados:  \n",
    "            juegos_relacionados = np.nan\n",
    "    except:\n",
    "        juegos_relacionados= np.nan\n",
    "    json_juego= {\"Nombre\": nombre_juego, \"Fecha publicación\": fecha_publicacion, \"Autores\": autores, \"Categorías\": categorias, \"Mecánicas\": mecanicas, \"Puntuación\": puntuacion, \"Jugadores\": jugadores, \"Jugadores mínimos\": jugadores_minimos, \"Jugadores máximos\": jugadores_maximos, \"Duración\": duracion_juego, \"Duración mínima\": duracion_minima, \"Duración máxima\": duracion_maxima, \"Edad\": edad, \"Complejidad\": complejidad, \"Dep idioma\": dep_idioma, \"Edición juegos\": edicion_juegos, \"Juegos relacionados\": juegos_relacionados}\n",
    "\n",
    "    return json_juego"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXTRACCIÓN DE LOS PRECIOS, TIENDAS Y DISPONIBILIDAD DE LOS JUEGOS\n",
    "\n",
    "La función precio_juegos(soup) toma el objeto soup que representa la página web de cada juego de Ludonauta, y después, busca y extrae información sobre los precios y la disponibilidad del juego en diferentes tiendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precio_juegos(soup):\n",
    "    \"\"\"Extrae de los juegos: lista de precios, lista de tiendas y su disponibilidad,\n",
    "    y lo guarda en json_precios\"\"\"\n",
    "    try:\n",
    "        precios_principales= soup.find_all(\"span\", class_= \"product-price btn btn-sm btn-primary\")\n",
    "        lista_precios_1=[]\n",
    "        for precio in precios_principales:\n",
    "            precios_tiendas_principales= precio.text\n",
    "            precios_principales= re.findall(r\"\\d+\\,\\d+\", precios_tiendas_principales)\n",
    "            lista_precios_1.extend(precios_principales)\n",
    "\n",
    "        precios_secundarios= soup.find_all(\"span\", class_= \"product-price btn btn-sm btn-default\")\n",
    "        lista_precios_2=[]\n",
    "        for precio in precios_secundarios:\n",
    "            precios_tiendas_secundarios= precio.text\n",
    "            precios_secundarios= re.findall(r\"\\d+\\,\\d+\", precios_tiendas_secundarios)\n",
    "            lista_precios_2.extend(precios_secundarios)\n",
    "           \n",
    "        lista_precios= lista_precios_1 + lista_precios_2\n",
    "        if not lista_precios:\n",
    "            lista_precios= np.nan\n",
    "    except:\n",
    "        lista_precios= np.nan\n",
    "    try:\n",
    "        tiendas= soup.find_all(\"a\", class_=\"visible-xxs-inline-block visible-xs-inline-block visible-sm-inline-block visible-md-inline-block visible-lg-inline-block visible-xl-inline-block\" )\n",
    "        lista_tiendas=[]\n",
    "        for tienda in tiendas:\n",
    "            texto_tiendas = tienda.get('title')\n",
    "            nombre_tienda = re.findall(r\"«(.*?)»\", texto_tiendas)\n",
    "            lista_tiendas.extend(nombre_tienda)\n",
    "            lista_tiendas = lista_tiendas[:len(lista_precios)]\n",
    "            if not lista_tiendas:\n",
    "                lista_tiendas= np.nan\n",
    "    except:\n",
    "        lista_tiendas= np.nan\n",
    "    try:\n",
    "        stock_contenedor= soup.find_all('td', class_='text-center')\n",
    "        disponibilidad_juego= []\n",
    "        for stock in stock_contenedor:\n",
    "            en_stock= stock.text.replace('\\n', '').strip()\n",
    "            if en_stock: \n",
    "                disponibilidad_juego.append(en_stock)\n",
    "                disponibilidad_juego= disponibilidad_juego[:len(lista_precios)]\n",
    "                if not disponibilidad_juego:\n",
    "                    disponibilidad_juego= np.nan\n",
    "    except:\n",
    "        disponibilidad_juego= np.nan\n",
    "    \n",
    "    json_precios={\"Nombre tienda\": lista_tiendas,\"Precio\": lista_precios, \"Disponibilidad\": disponibilidad_juego}\n",
    "\n",
    "    return json_precios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUTOMATIZACIÓN DE LA EXTRACCIÓN DE DATOS \n",
    "\n",
    "Este script utiliza Selenium y BeautifulSoup para navegar a través de la lista de páginas web, extraer enlaces, acceder a cada enlace, y extraer información sobre juegos y precios. Los resultados se almacenan en diccionarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = webdriver.Firefox()    \n",
    "diccionario_juegos= {}\n",
    "diccionario_precios= {}\n",
    "\n",
    "# BUCLE PARA ACCEDER A CADA PÁGINA\n",
    "for i in range(0,379): # Número de páginas de la web\n",
    "    cada_pagina= browser.get(listado_paginas[i])                                                        \n",
    "    soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "    sleep(2)\n",
    "    enlaces = extraer_href_pagina(soup)\n",
    "    \n",
    "    # EXTRACCIÓN DEL LINK DE LOS JUEGOS DE CADA PAGINA\n",
    "    for enlace in enlaces:\n",
    "        cada_enlace = browser.get(enlace)\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        \n",
    "        # EXTRACCIÓN DE LA INFORMACIÓN DE CADA JUEGO \n",
    "        extraccion_juego= extraer_juego(soup)\n",
    "        extraccion_precio= precio_juegos(soup)\n",
    "            \n",
    "        diccionario_juegos[enlace] = extraccion_juego\n",
    "        diccionario_precios[enlace] = extraccion_precio\n",
    "\n",
    "    sleep(2)\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV DE JUEGOS\n",
    "\n",
    "Se observa que ha recogido todos los datos y después, con pandas, se convierte diccionario_juegos en un DataFrame y se guarda en un archivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(diccionario_juegos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_juegos = pd.DataFrame.from_dict(diccionario_juegos, orient='index')\n",
    "df_juegos.to_csv(\"juegos_ludonauta.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV DE PRECIOS\n",
    "\n",
    "Se observa que ha recogido bien los precios, se convierte diccionario_precios en un DataFrame y se guarda en un CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(diccionario_precios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_precios = pd.DataFrame.from_dict(diccionario_precios, orient='index')\n",
    "df_precios.to_csv(\"precios_ludonauta.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
